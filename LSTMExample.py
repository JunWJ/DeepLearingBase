import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import time
from sklearn.preprocessing import MinMaxScaler
import platform

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ===================== å…¨å±€ç¯å¢ƒé…ç½® =====================
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('medium')

# ===================== æŒ‡æ ‡è®¡ç®—å·¥å…·å‡½æ•° =====================
def calculate_rmse(y_true, y_pred):
    """è®¡ç®—å‡æ–¹æ ¹è¯¯å·® (RMSE)"""
    return np.sqrt(np.mean((y_true - y_pred) ** 2))

def calculate_mape(y_true, y_pred, epsilon=1e-8):
    """è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE)ï¼Œè¿”å›ç™¾åˆ†æ¯”å½¢å¼"""
    y_true = np.where(y_true == 0, epsilon, y_true)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return mape

def inverse_transform_predictions(y_pred, y_true, scaler):
    """åå½’ä¸€åŒ–é¢„æµ‹å€¼å’ŒçœŸå®å€¼"""
    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_true_inv = scaler.inverse_transform(y_true.reshape(-1, 1))
    return y_pred_inv.flatten(), y_true_inv.flatten()

# ===================== æ•°æ®é¢„å¤„ç†ç±» =====================
class GoldPriceDataProcessor:
    """é»„é‡‘ä»·æ ¼æ•°æ®é¢„å¤„ç†ç±»"""
    def __init__(self, data_path, time_step=10, train_test_split=200):
        self.data_path = data_path
        self.time_step = time_step
        self.train_test_split = train_test_split
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.dataSet = None
        self.train_data = None
        self.test_data = None
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        self._setup_chinese_font()
    
    def _setup_chinese_font(self):
        """è®¾ç½®Matplotlibæ”¯æŒä¸­æ–‡"""
        try:
            system = platform.system()
            font_configs = {
                "Windows": ['Microsoft YaHei', 'SimHei'],
                "Linux": ['WenQuanYi Zen Hei'],
                "Darwin": ['Arial Unicode MS', 'PingFang SC']
            }
            plt.rcParams['font.sans-serif'] = font_configs.get(system, ['DejaVu Sans'])
            plt.rcParams['axes.unicode_minus'] = False
        except Exception as e:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
    
    def load_data(self):
        """åŠ è½½CSVæ•°æ®ï¼Œå¢åŠ å¼‚å¸¸å¤„ç†"""
        try:
            self.dataSet = pd.read_csv(self.data_path, index_col=[0])
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.dataSet.shape}")
            return self.dataSet
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ {self.data_path}")
            raise
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}")
            raise
    
    def split_train_test(self):
        """åˆ’åˆ†å¹¶å½’ä¸€åŒ–è®­ç»ƒ/æµ‹è¯•é›†"""
        if self.dataSet is None:
            self.load_data()
        
        train_len = len(self.dataSet) - self.train_test_split
        train_set = self.dataSet.iloc[:train_len, [0]]
        test_set = self.dataSet.iloc[train_len:, [0]]
        
        self.train_data = self.scaler.fit_transform(train_set)
        self.test_data = self.scaler.transform(test_set)
        return self.train_data, self.test_data
    
    def _create_sequences(self, data):
        """åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®"""
        x, y = [], []
        for i in range(self.time_step, len(data)):
            x.append(data[i-self.time_step:i, 0])
            y.append(data[i, 0])
        
        x = np.array(x).reshape(-1, self.time_step, 1)
        y = np.array(y).reshape(-1, 1)
        return x, y
    
    def get_processed_data(self):
        """è·å–æ‰€æœ‰é¢„å¤„ç†åçš„æ•°æ®ï¼ˆè½¬ä¸ºTensorï¼‰"""
        self.split_train_test()
        x_train, y_train = self._create_sequences(self.train_data)
        x_test, y_test = self._create_sequences(self.test_data)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        self.x_train = torch.tensor(x_train, dtype=torch.float32)
        self.y_train = torch.tensor(y_train, dtype=torch.float32)
        self.x_test = torch.tensor(x_test, dtype=torch.float32)
        self.y_test = torch.tensor(y_test, dtype=torch.float32)
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"   - è®­ç»ƒé›†: {self.x_train.shape} | æµ‹è¯•é›†: {self.x_test.shape}")
        return self.x_train, self.y_train, self.x_test, self.y_test

# ===================== LSTMæ¨¡å‹ç±»ï¼ˆä¿®å¤dropoutè­¦å‘Šï¼‰ =====================
class GoldPriceLSTM(nn.Module):
    """é»„é‡‘ä»·æ ¼é¢„æµ‹çš„LSTMæ¨¡å‹ï¼ˆåŒ¹é…ç»å…¸åŒå±‚ç»“æ„ï¼‰"""
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, dropout=0.1):
        super(GoldPriceLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # ç¬¬ä¸€å±‚LSTMï¼šè¿”å›åºåˆ—ï¼ˆreturn_sequences=Trueï¼‰ï¼Œå•å±‚ä¸è®¾ç½®dropout
        self.lstm1 = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )
        
        # ç¬¬äºŒå±‚LSTMï¼šä¸è¿”å›åºåˆ—ï¼Œå•å±‚ä¸è®¾ç½®dropoutï¼ˆä¿®å¤è­¦å‘Šï¼‰
        self.lstm2 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)  # å•ç‹¬çš„dropoutå±‚
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€å±‚LSTMï¼šè¾“å‡º (batch_size, seq_len, hidden_size)
        lstm1_out, _ = self.lstm1(x)
        lstm1_out = self.dropout(lstm1_out)
        
        # ç¬¬äºŒå±‚LSTMï¼šè¾“å‡º (batch_size, 1, hidden_size)
        lstm2_out, _ = self.lstm2(lstm1_out)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = lstm2_out[:, -1, :]
        
        # å…¨è¿æ¥å±‚è¾“å‡º
        out = self.fc(out)
        return out

# ===================== è®­ç»ƒå™¨ç±»ï¼ˆæ”¹ç”¨Adamä¼˜åŒ–å™¨ï¼‰ =====================
class GoldPriceTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è®­+RMSE/MAPEç›‘æ§ï¼‰"""
    def __init__(self, model, scaler, device, save_path="results/goldPredict/gold_price_model.pth"):
        self.model = model
        self.scaler = scaler  # ä¿å­˜å½’ä¸€åŒ–å™¨ç”¨äºæŒ‡æ ‡è®¡ç®—
        self.device = device
        self.save_path = save_path
        self.model.to(device)
        
        # è®­ç»ƒè®°å½•ï¼ˆæ‰©å±•æŒ‡æ ‡ï¼‰
        self.train_losses = []
        self.val_losses = []
        self.train_rmse = []
        self.val_rmse = []
        self.train_mape = []
        self.val_mape = []
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
    
    def _save_checkpoint(self, epoch, optimizer, loss):
        """ä¿å­˜è®­ç»ƒæ–­ç‚¹ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ ‡ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_rmse': self.train_rmse,
            'val_rmse': self.val_rmse,
            'train_mape': self.train_mape,
            'val_mape': self.val_mape,
            'loss': loss
        }
        torch.save(checkpoint, self.save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {self.save_path} (Epoch: {epoch})")
    
    def _load_checkpoint(self, optimizer):
        """åŠ è½½è®­ç»ƒæ–­ç‚¹ï¼ˆæ‰‹åŠ¨å¤„ç†è®¾å¤‡é—®é¢˜ï¼‰"""
        if os.path.exists(self.save_path):
            checkpoint = torch.load(self.save_path, map_location='cpu')  # å…ˆåŠ è½½åˆ°CPU
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)  # å†ç§»åˆ°ç›®æ ‡è®¾å¤‡
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€åˆ°CPUï¼Œå†ç§»åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆé¿å…CUDAæ–­è¨€é”™è¯¯ï¼‰
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            for state in optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(self.device)
            
            start_epoch = checkpoint['epoch'] + 1
            self.train_losses = checkpoint['train_losses']
            self.val_losses = checkpoint['val_losses']
            self.train_rmse = checkpoint['train_rmse']
            self.val_rmse = checkpoint['val_rmse']
            self.train_mape = checkpoint['train_mape']
            self.val_mape = checkpoint['val_mape']
            
            print(f"âœ… åŠ è½½æ–­ç‚¹æˆåŠŸï¼Œä»Epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
            return start_epoch
        else:
            print("âš ï¸  æœªæ‰¾åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            return 0
    
    def train(self, train_loader, val_loader, epochs=100, lr=0.001, patience=10):
        """
        æ¨¡å‹è®­ç»ƒï¼ˆæ”¯æŒæ—©åœå’Œæ–­ç‚¹ç»­è®­ï¼Œç›‘æ§RMSE/MAPEï¼‰
        """
        # ========== æ”¹ç”¨Adamä¼˜åŒ–å™¨ï¼ˆç§»é™¤capturableå‚æ•°ï¼Œé€‚é…æ—§ç‰ˆæœ¬PyTorchï¼‰ ==========
        optimizer = optim.Adam(
            self.model.parameters(), 
            lr=lr,
            betas=(0.9, 0.999),  # é»˜è®¤å€¼ï¼Œæ˜¾å¼å£°æ˜å¢åŠ å…¼å®¹æ€§
            eps=1e-08
        )
        criterion = nn.MSELoss()
        
        # åŠ è½½æ–­ç‚¹
        start_epoch = self._load_checkpoint(optimizer)
        
        # æ—©åœç›¸å…³ï¼ˆç”¨RMSEä½œä¸ºæ—©åœæŒ‡æ ‡ï¼‰
        best_val_rmse = float('inf')
        patience_counter = 0
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (æ€»è½®æ•°: {epochs}, èµ·å§‹è½®æ•°: {start_epoch})")
        print("-" * 90)
        print(f"{'Epoch':<6} {'Train Loss':<12} {'Val Loss':<12} {'Train RMSE':<12} {'Val RMSE':<12} {'Train MAPE(%)':<12} {'Val MAPE(%)':<12} {'Time(s)':<8}")
        print("-" * 90)
        
        for epoch in range(start_epoch, epochs):
            start_time = time.time()
            
            # ========== è®­ç»ƒé˜¶æ®µ ==========
            self.model.train()
            train_loss = 0.0
            train_preds = []
            train_targets = []
            
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item() * batch_x.size(0)
                # æ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼
                train_preds.extend(outputs.detach().cpu().numpy())
                train_targets.extend(batch_y.detach().cpu().numpy())
            
            # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
            avg_train_loss = train_loss / len(train_loader.dataset)
            train_preds, train_targets = inverse_transform_predictions(
                np.array(train_preds), np.array(train_targets), self.scaler
            )
            train_rmse = calculate_rmse(train_targets, train_preds)
            train_mape = calculate_mape(train_targets, train_preds)
            
            # ========== éªŒè¯é˜¶æ®µ ==========
            self.model.eval()
            val_loss = 0.0
            val_preds = []
            val_targets = []
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    
                    val_loss += loss.item() * batch_x.size(0)
                    val_preds.extend(outputs.cpu().numpy())
                    val_targets.extend(batch_y.cpu().numpy())
            
            # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
            avg_val_loss = val_loss / len(val_loader.dataset)
            val_preds, val_targets = inverse_transform_predictions(
                np.array(val_preds), np.array(val_targets), self.scaler
            )
            val_rmse = calculate_rmse(val_targets, val_preds)
            val_mape = calculate_mape(val_targets, val_preds)
            
            # ========== è®°å½•ä¸æ‰“å° ==========
            self.train_losses.append(avg_train_loss)
            self.val_losses.append(avg_val_loss)
            self.train_rmse.append(train_rmse)
            self.val_rmse.append(val_rmse)
            self.train_mape.append(train_mape)
            self.val_mape.append(val_mape)
            
            epoch_time = time.time() - start_time
            print(f"{epoch+1:<6} {avg_train_loss:<12.6f} {avg_val_loss:<12.6f} "
                  f"{train_rmse:<12.2f} {val_rmse:<12.2f} {train_mape:<12.2f} {val_mape:<12.2f} {epoch_time:<8.2f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºRMSEï¼‰
            if val_rmse < best_val_rmse:
                best_val_rmse = val_rmse
                self._save_checkpoint(epoch, optimizer, avg_val_loss)
                patience_counter = 0
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ (Patience: {patience})")
                break
        
        print("-" * 90)
        print("\nğŸ è®­ç»ƒå®Œæˆï¼")
    
    def plot_training_metrics(self):
        """å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡ï¼ˆLoss + RMSE + MAPEï¼‰"""
        fig, axes = plt.subplots(3, 1, figsize=(14, 12))
        
        # 1. æŸå¤±æ›²çº¿
        axes[0].plot(range(1, len(self.train_losses)+1), self.train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2)
        axes[0].plot(range(1, len(self.val_losses)+1), self.val_losses, label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)
        axes[0].set_ylabel('æŸå¤±å€¼ (MSE)', fontsize=12)
        axes[0].set_title('é»„é‡‘ä»·æ ¼LSTMæ¨¡å‹ - æŸå¤±æ›²çº¿', fontsize=14)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. RMSEæ›²çº¿
        axes[1].plot(range(1, len(self.train_rmse)+1), self.train_rmse, label='è®­ç»ƒRMSE', linewidth=2)
        axes[1].plot(range(1, len(self.val_rmse)+1), self.val_rmse, label='éªŒè¯RMSE', linewidth=2)
        axes[1].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)
        axes[1].set_ylabel('RMSE (USD)', fontsize=12)
        axes[1].set_title('é»„é‡‘ä»·æ ¼LSTMæ¨¡å‹ - RMSEæ›²çº¿', fontsize=14)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. MAPEæ›²çº¿
        axes[2].plot(range(1, len(self.train_mape)+1), self.train_mape, label='è®­ç»ƒMAPE', linewidth=2)
        axes[2].plot(range(1, len(self.val_mape)+1), self.val_mape, label='éªŒè¯MAPE', linewidth=2)
        axes[2].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)
        axes[2].set_ylabel('MAPE (%)', fontsize=12)
        axes[2].set_title('é»„é‡‘ä»·æ ¼LSTMæ¨¡å‹ - MAPEæ›²çº¿', fontsize=14)
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/goldPredict/training_metrics.png', dpi=300, bbox_inches='tight')
        plt.show()

# ===================== æµ‹è¯•ä¸å¯è§†åŒ– =====================
def test_model(model, x_test, y_test, scaler, device):
    """æ¨¡å‹æµ‹è¯•ä¸ç»“æœå¯è§†åŒ–ï¼ˆè®¡ç®—RMSE/MAPEï¼‰"""
    model.eval()
    test_preds = []
    test_targets = []
    
    with torch.no_grad():
        # åˆ†æ‰¹é¢„æµ‹é¿å…å†…å­˜æº¢å‡º
        for i in range(0, len(x_test), 32):
            batch_x = x_test[i:i+32].to(device)
            batch_y = y_test[i:i+32].cpu().numpy()
            
            outputs = model(batch_x)
            test_preds.extend(outputs.cpu().numpy())
            test_targets.extend(batch_y)
    
    # åå½’ä¸€åŒ–
    test_preds, test_targets = inverse_transform_predictions(
        np.array(test_preds), np.array(test_targets), scaler
    )
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = np.mean(np.abs(test_targets - test_preds))
    rmse = calculate_rmse(test_targets, test_preds)
    mape = calculate_mape(test_targets, test_preds)
    
    print(f"\nğŸ“Š æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   - å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.2f} USD")
    print(f"   - å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.2f} USD")
    print(f"   - å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {mape:.2f} %")
    
    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    plt.figure(figsize=(14, 7))
    plt.plot(test_targets, label='çœŸå®ä»·æ ¼', linewidth=2)
    plt.plot(test_preds, label='é¢„æµ‹ä»·æ ¼', linewidth=2, alpha=0.8)
    plt.xlabel('æ—¶é—´æ­¥', fontsize=12)
    plt.ylabel('é»„é‡‘ä»·æ ¼ (USD)', fontsize=12)
    plt.title(f'é»„é‡‘ä»·æ ¼é¢„æµ‹ç»“æœå¯¹æ¯” (RMSE: {rmse:.2f} | MAPE: {mape:.2f}%)', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('results/goldPredict/prediction_result.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return test_preds, mae, rmse, mape

# ===================== ä¸»å‡½æ•° =====================
def main():
    # 1. é…ç½®å‚æ•°
    DATA_PATH = "resources/LBMA-GOLD.csv"
    TIME_STEP = 5               # æ—¶é—´æ­¥é•¿
    BATCH_SIZE = 32             # æ‰¹æ¬¡å¤§å°
    EPOCHS = 1000               # è®­ç»ƒè½®æ•°
    LR = 1e-4                   # Adamæ¨èå­¦ä¹ ç‡
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    processor = GoldPriceDataProcessor(DATA_PATH, time_step=TIME_STEP)
    x_train, y_train, x_test, y_test = processor.get_processed_data()
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(x_train, y_train)
    val_dataset = TensorDataset(x_test, y_test)
    
    # è®­ç»ƒé›†shuffle=Trueï¼ˆæ‰“ä¹±æ ·æœ¬é—´é¡ºåºï¼Œä¿ç•™æ ·æœ¬å†…æ—¶åºï¼‰
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    # éªŒè¯é›†shuffle=Falseï¼ˆä¿æŒæ—¶é—´é¡ºåºï¼‰
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    model = GoldPriceLSTM(hidden_size=50, num_layers=2)
    print(f"\nğŸ§  æ¨¡å‹ç»“æ„: {model}")
    
    # 5. è®­ç»ƒæ¨¡å‹ï¼ˆä¼ å…¥scalerç”¨äºæŒ‡æ ‡è®¡ç®—ï¼‰
    trainer = GoldPriceTrainer(model, processor.scaler, DEVICE)
    trainer.train(train_loader, val_loader, epochs=EPOCHS, lr=LR, patience=20)
    
    # 6. å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡
    trainer.plot_training_metrics()
    
    # 7. æµ‹è¯•æ¨¡å‹
    test_model(model, x_test, y_test, processor.scaler, DEVICE)

if __name__ == "__main__":
    main()